spinning up the process as we begin the project, we create react app, ver 13.5.3
  npx create-react-app@13.5.3

choosing the configs, we simply run 
  npm run dev

removing the existing template, we dont need:
rafce page.tsx in app,
editing global.css
same with tailwind.config.ts
and updating fonts in layout.tsx, adding weights
adding navbar component in the body of the layout.tsx

creating navbar as welll placing it inside components, which we later import in the layout.tsx 

we added navbar in the layout itself, cause navbars, headers and footers all usually stay the same through the entire website so we can just share them instead of always having to create it again n again :)

later we reconfig the public folder with new assets containing images and icons, gonna add them as we first use them.

adding stylings and actually creating navbar now.
working in the navbas.jsx file

commit --

next we create the home.tsx page, adding the head section for the homepage, heroCarousel, searchBar, and the Trending section text marks, that will be build step wise step one by one

commit --

moving on to the searchBar first, creating searchbar.tsx component and updating its UI.
next we move to the heroCarousel, working on its UI: 
  for this we use: 
    npm install react-responsive-carousel

https://www.npmjs.com/package/react-responsive-carousel

adding code from the usage code snippet in the website

we reconfigure the images to the images we gonna add from local machine and add props to the carousel now, referencing how to do that from the link above.

again, the user can interact with this carousel, hence, we need to add "use client" on the top in order to fix all the errors :)

next we add a arrow pointing towards the link text entry field, only visible when te website is landscape-ish

adding the interaction functionality of the serachBar
making it check if the url entered belongs to the hostname amazon or not
and disbaling the button if the field is empty.

commit --

to enable web scraping, we will now work on the server side section, implimenting server side actions, 
  lib -> actions -> index.ts 

try catch block is used to handle async functions.

next for the scraper(we will be using bright data), we form:
  lib -> scraper -> index.ts 

next we head to https://brightdata.com/ create account and choose proxy scraping Infrastructure and under it we select web unblocker

and we set it up by giving it the name pricefries and adding the web scraper
https://brightdata.com/cp/zones/pricefries/access_params

now for the credentials to be stored we create a .env as we add info to the index.ts for scraper.

get the port number from the link: 22225
host here is brd.superproxy.io
<!-- curl --proxy brd.superproxy.io:22225 --proxy-user brd-customer-hl_cd149885-zone-pricefries:icj2vo6gshhq -k "http://geo.brdtest.com/mygeo.json" -->

generate a sessionID by: 
  const session_id = (1000000 * Math.random()) | 0;


to fetch the amazon product we need to scrape, we need to fetch it which can be done using axios, with cheerio:
  npm install axios cheerio

https://cheerio.js.org/ 

parsing is import in scraping, cause we need only the stuff we want from the scraped info i.e parsed content, which is made easier by cheerio.

now we add the scraping functionality in the searchbar, setting up the backend of it.
  components -> searchBar.tsx

tesing it out taking this as example:
  https://www.amazon.in/Fire-Boltt-Stainless-Resolution-Brightness-Bluetooth/dp/B0C49DCRN7/?_encoding=UTF8&ref_=pd_hp_d_atf_ci_mcx_mr_hp_atf_m

updating the next.config.js to fix the network error, found while testing the scraper.

error fixed. got the output in console (pretty big damn)
so to parse it, we add cheerio.

need to specify the class or id, of which information we want, so we go to amazon and inspect the website in order to know what we need.

to find the price of product in the midst of so much span elements we create a utility function under lib 
  lib -> utils.ts

now we can fetch the title, current price and original price of the product, need to fix parsing tho in the extractPrice function under utils.

to get to know more about regular expressions we can just chatgpt 
<!-- if(priceText) return priceText.replace(/[^\d.]/g, ''); -->

for images, we get a whole list of objects, but we only need keysfrom them, so we need to parse them.

next to get currency we create a separate function extractCurrency inside the same utils.ts

next we add discountrate of the product, to determine the best time to buy the product

category, reviewCounts, stars, to be added later.

commit --

now we construct data object with scraped info, instead of logging it to console:
in the data object, we reference all the fetched info accordingly and will also create a list/array to store all the prices at different times.

commit --

refreshing the utils.ts with proper most possible up to date parsing scheme,

next we to specify the types of content we wanna extract, since we are using typescript, so we create: 
  -> types -> index.ts 

here we add all the types we will be using, examples are product, notification, user, price-history item...

now in the data object, we add description(very difficult to parse entirely), lowestPrice, highestPrice and averagePrice, all important to identify which would be the best time to buuy the product.

after checking, instead of logging, we gonna return the data

and this data is parsed scraped data of the product, now we need to store it on our database, hence updating: 
  libs -> actions -> index.ts 
to add whats to be done with the data and storing it in database. 

commit -- 

next we adding the mongoose.ts for backend and setting up connection to db.

so we need mongoose package added:
  npm install mongoose

(using aaryank098@gmail.com(google acc)) for the db this time.
adding the connectToDB() exported from mongoose.ts into 
  actions -> index.ts

coneccted successfully

now we will track the prices of the product and store it and its detail into our db, but what kind of thing we would be storing in the db ? 

for that we need to create models/schema for the db.
  libs -> models -> 


always need to revalidate the path, or its gonna stay in cache.
so we use revalidatePath inside 
  actions -> index.ts

in the same, we gonna create a new function getProductById

now we will add the products route, displaying all of these to the front end

commit -- 

to create a new route in nextjs, that follows a file system page, we create:
  app -> products -> [id] -> page.tsx

to get all products, we will create another action getAllProducts() in the      actions -> index.ts
next we import the action function in the app page.tsx

but we dont just wanna show title, we want cards for these products, hence new component created:

to get the id of the product we just opened, we will use params in Products 
page.tsx

forgot to add: 
  return product 
in the async function getProductById() in the actions -> index.ts file (ahh spend a lot of time trying to find the error TwT)

commit --

next for the priceInfo we, create card component.

in order to fetch similar products, we will need to create another action in the
libs -> actions -> index.ts

commit -- 

for the website to scrape automatically and update the average lowest and highest prices periodically we will be introducing cron jobs, but first we create Modal component (pop up menu if the user wants to recieve email regarding lowest price).

for the dialog box we will be using headless ui components:
  npm install @headlessui/react
https://headlessui.com/react/dialog

next we need to handle the email input by the user so we can mail them about the prices later.

we got the email input now, so we need to handle the action to save the user email with the productId that the user wants to keep a track of, so:

  lib -> actions -> index.ts

for the mails to work realtime, we will create: 

  lib -> nodemailer -> index.ts

and we install the package: 
  npm install nodemailer


since to send mails using google, have to step up oAuth which can be really strechy, we will be using hotmail with nodemailer

https://stackoverflow.com/questions/75414959/using-nodemailer-with-hotmail-node-js

need to create an outlook account, 


to fix the nodemailer error, need to add it as dev dependency:
  npm install --save-dev @types/nodemailer


for the cron to work, we will be using its api which will be under: 
  app -> api -> cron -> route.ts

adding the steps required for the cron job to follow the steps of:

  step-1. scrape latest product details and update db.
  step-2. now we will check the status of each products and send email accordingly

tried adding more scraping params, like stars and reviesCounts:
  https://crawlee.dev/blog/how-to-scrape-amazon

still doesnt seem to work so changing it as a link to them.

commit --
  



